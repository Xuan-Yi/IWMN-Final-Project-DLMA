{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaffb2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T19:23:35.060999Z",
     "iopub.status.busy": "2023-06-14T19:23:35.060569Z",
     "iopub.status.idle": "2023-06-14T19:23:43.188468Z",
     "shell.execute_reply": "2023-06-14T19:23:43.187376Z"
    },
    "id": "ZH0EFBY1Q2qF",
    "papermill": {
     "duration": 8.138434,
     "end_time": "2023-06-14T19:23:43.191548",
     "exception": false,
     "start_time": "2023-06-14T19:23:35.053114",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Add\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.initializers import glorot_normal\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c0bce2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T19:23:43.203453Z",
     "iopub.status.busy": "2023-06-14T19:23:43.202835Z",
     "iopub.status.idle": "2023-06-14T19:23:43.207080Z",
     "shell.execute_reply": "2023-06-14T19:23:43.206245Z"
    },
    "id": "rNfH9X9nCQZj",
    "papermill": {
     "duration": 0.012306,
     "end_time": "2023-06-14T19:23:43.209128",
     "exception": false,
     "start_time": "2023-06-14T19:23:43.196822",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = '2 agents'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb2860ec",
   "metadata": {
    "id": "6CcCuj8JQEA_",
    "papermill": {
     "duration": 0.004806,
     "end_time": "2023-06-14T19:23:43.218811",
     "exception": false,
     "start_time": "2023-06-14T19:23:43.214005",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Fix random seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e971e212",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T19:23:43.229595Z",
     "iopub.status.busy": "2023-06-14T19:23:43.229290Z",
     "iopub.status.idle": "2023-06-14T19:23:43.234076Z",
     "shell.execute_reply": "2023-06-14T19:23:43.233100Z"
    },
    "id": "M5ivJuAQQCx3",
    "papermill": {
     "duration": 0.012309,
     "end_time": "2023-06-14T19:23:43.235958",
     "exception": false,
     "start_time": "2023-06-14T19:23:43.223649",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def same_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "same_seeds(48763)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd8b6037",
   "metadata": {
    "id": "7dlFkYONQ2qJ",
    "papermill": {
     "duration": 0.004748,
     "end_time": "2023-06-14T19:23:43.245575",
     "exception": false,
     "start_time": "2023-06-14T19:23:43.240827",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Protocols\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7885fc5f",
   "metadata": {
    "papermill": {
     "duration": 0.004673,
     "end_time": "2023-06-14T19:23:43.255086",
     "exception": false,
     "start_time": "2023-06-14T19:23:43.250413",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49b6487",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T19:23:43.266482Z",
     "iopub.status.busy": "2023-06-14T19:23:43.266187Z",
     "iopub.status.idle": "2023-06-14T19:23:43.286954Z",
     "shell.execute_reply": "2023-06-14T19:23:43.286052Z"
    },
    "id": "C0DeJwXCQ2qK",
    "papermill": {
     "duration": 0.028631,
     "end_time": "2023-06-14T19:23:43.288834",
     "exception": false,
     "start_time": "2023-06-14T19:23:43.260203",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self,\n",
    "                 state_size,\n",
    "                 n_nodes,  # N: number of all nodes\n",
    "                 n_actions,\n",
    "                 memory_size=500,\n",
    "                 replace_target_iter=200,\n",
    "                 batch_size=32,\n",
    "                 learning_rate=0.01,\n",
    "                 gamma=0.9,\n",
    "                 epsilon=1,\n",
    "                 epsilon_min=0.01,\n",
    "                 epsilon_decay=0.995,\n",
    "                 alpha=0  # 0 ~ 100 (inf)\n",
    "                 ):\n",
    "        # hyper-parameters\n",
    "        self.state_size = state_size\n",
    "        self.n_nodes = n_nodes\n",
    "        self.n_actions = n_actions\n",
    "        self.memory_size = memory_size\n",
    "        self.replace_target_iter = replace_target_iter\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.zeros(self.state_size)  # init state\n",
    "        self.n_iter = 0  # current iteration\n",
    "\n",
    "        # [s, a, r1, r2, ..., s_]\n",
    "        self.memory = np.zeros(\n",
    "            shape=(self.memory_size, self.state_size * 2 + (self.n_nodes + 1)))\n",
    "        # temporary parameters\n",
    "        self.learn_step_counter = 0\n",
    "        self.memory_couter = 0\n",
    "\n",
    "        # build model\n",
    "        self.model = self.__build_ResNet_model__()  # model: evaluate Q value\n",
    "        self.target_model = self.__build_ResNet_model__()  # target_mode: target network\n",
    "\n",
    "    def tic(self):\n",
    "        self.n_iter += 1\n",
    "        self.agent_action = np.array(\n",
    "            self.__choose_action__(self.state), dtype=np.float32)\n",
    "\n",
    "        return self.agent_action\n",
    "\n",
    "    def update(self, observation_, agent_reward, non_agent_reward):\n",
    "        # non_agent_reward: 1D array or scalar\n",
    "        next_state = np.concatenate((self.state[8:], np.array(self.__return_action__(\n",
    "            self.agent_action) + self.__return_observation__(observation_) + [agent_reward, np.sum(non_agent_reward, dtype=np.float32)], dtype=np.float32)))\n",
    "\n",
    "        self.__store_transition__(\n",
    "            self.state, self.agent_action, agent_reward, non_agent_reward, next_state)\n",
    "\n",
    "        if self.n_iter > 100:\n",
    "            self.__learn__()    # internally iterates default (prediction) model\n",
    "\n",
    "        self.state = next_state\n",
    "\n",
    "    def __return_action__(self, action):\n",
    "        one_hot_vector = [0] * self.n_actions\n",
    "        one_hot_vector[int(action)] = 1\n",
    "        return one_hot_vector\n",
    "\n",
    "    def __return_observation__(self, o):\n",
    "        if o == 'S':\n",
    "            return [1, 0, 0, 0]\n",
    "        elif o == 'F':\n",
    "            return [0, 1, 0, 0]\n",
    "        elif o == 'B':\n",
    "            return [0, 0, 1, 0]\n",
    "        elif o == 'I':\n",
    "            return [0, 0, 0, 1]\n",
    "        else:\n",
    "            print(f'error obervation: {o}')\n",
    "\n",
    "    def __alpha_function__(self, action_values):\n",
    "        if self.alpha == 1:\n",
    "            log_action_values = np.log(action_values, dtype=np.float32)\n",
    "            action_values_list = [np.sum(log_action_values[self.n_nodes*j: self.n_nodes*(\n",
    "                j+1)], dtype=np.float32) for j in range(self.n_actions)]\n",
    "        elif self.alpha == 0:\n",
    "            action_values_list = [np.sum(action_values[self.n_nodes*j: self.n_nodes*(\n",
    "                j+1)], dtype=np.float32) for j in range(self.n_actions)]\n",
    "        elif self.alpha == 100:\n",
    "            action_values_list = [\n",
    "                np.amin(action_values[self.n_nodes*j: self.n_nodes*(j+1)], axis=0) for j in range(self.n_actions)]\n",
    "        else:\n",
    "            pow_action_values = np.power(\n",
    "                action_values, (1-self.alpha), dtype=np.float32)\n",
    "            action_values_list = [1/(1-self.alpha) * np.sum(pow_action_values[self.n_nodes *\n",
    "                                                                              j: self.n_nodes*(j+1)], dtype=np.float32) for j in range(self.n_actions)]\n",
    "\n",
    "        return np.argmax(action_values_list)\n",
    "\n",
    "    def __build_ResNet_model__(self):\n",
    "        inputs = Input(shape=(self.state_size, ))\n",
    "        h1 = Dense(64, activation=\"relu\",\n",
    "                   kernel_initializer='glorot_normal')(inputs)  # h1\n",
    "        h2 = Dense(64, activation=\"relu\",\n",
    "                   kernel_initializer='glorot_normal')(h1)  # h2\n",
    "\n",
    "        h3 = Dense(64, activation=\"relu\",\n",
    "                   kernel_initializer='glorot_normal')(h2)  # h3\n",
    "        h4 = Dense(64, activation=\"relu\",\n",
    "                   kernel_initializer='glorot_normal')(h3)  # h4\n",
    "        add1 = Add()([h4, h2])\n",
    "\n",
    "        h5 = Dense(64, activation=\"relu\",\n",
    "                   kernel_initializer='glorot_normal')(add1)  # h5\n",
    "        h6 = Dense(64, activation=\"relu\",\n",
    "                   kernel_initializer='glorot_normal')(h5)  # h6\n",
    "        add2 = Add()([h6, add1])\n",
    "\n",
    "        outputs = Dense(\n",
    "            self.n_actions*self.n_nodes, kernel_initializer='glorot_normal')(add2)\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(loss=\"mse\", optimizer=Adam(\n",
    "            learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def __choose_action__(self, state):\n",
    "        # Apply epsilon-greedy algorithm\n",
    "        state = state[np.newaxis, :]\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon)\n",
    "\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(0, self.n_actions)\n",
    "\n",
    "        action_values = self.model.predict(state, verbose=None)\n",
    "        return self.__alpha_function__(action_values[0])\n",
    "\n",
    "    def __store_transition__(self, s, a, r_dqn, r_non_dqn, s_):\n",
    "        # s_: next_state\n",
    "        if not hasattr(self, 'memory_couter'):\n",
    "            self.memory_couter = 0\n",
    "        transition = np.concatenate((s, [a, r_dqn], r_non_dqn, s_))\n",
    "        index = self.memory_couter % self.memory_size\n",
    "        self.memory[index, :] = transition\n",
    "        self.memory_couter += 1\n",
    "\n",
    "    def __repalce_target_parameters__(self):\n",
    "        weights = self.model.get_weights()\n",
    "        self.target_model.set_weights(weights)\n",
    "\n",
    "    def __learn__(self):\n",
    "        if self.learn_step_counter % self.replace_target_iter == 0:\n",
    "            self.__repalce_target_parameters__()  # iterative target model\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "        if self.memory_couter > self.memory_size:\n",
    "            sample_index = np.random.choice(\n",
    "                self.memory_size, size=self.batch_size)\n",
    "        else:\n",
    "            sample_index = np.random.choice(\n",
    "                self.memory_couter, size=self.batch_size)\n",
    "        batch_memory = self.memory[sample_index, :]\n",
    "\n",
    "        # batch memory row: [s, a, r1, r2, ..., s_]\n",
    "        state = batch_memory[:, :self.state_size]\n",
    "        action = batch_memory[:, self.state_size].astype(\n",
    "            np.int32)  # float -> int\n",
    "        rewards = batch_memory[:, self.state_size +\n",
    "                               1: self.state_size+self.n_nodes+1]  # [:, (r1, r2, ...)]\n",
    "        next_state = batch_memory[:, -self.state_size:]\n",
    "\n",
    "        q = self.model.predict(state, verbose=None)  # state\n",
    "        q_targ = self.target_model.predict(\n",
    "            next_state, verbose=None)  # next state\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            action_ = self.__alpha_function__(q_targ[i])\n",
    "\n",
    "            # action_:\n",
    "            # |      a0      |      a1      |\n",
    "            # | 01 | 02 | 03 | 01 | 02 | 03 |\n",
    "            for node in range(self.n_nodes):\n",
    "                q[i, self.n_nodes*action[i]+node] = rewards[i, node] + \\\n",
    "                    self.gamma*q_targ[i][self.n_nodes*action_+node]\n",
    "\n",
    "        self.model.fit(state, q, self.batch_size, epochs=1, verbose=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e32ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_NODES:\n",
    "    def __init__(self,\n",
    "                 state_size,\n",
    "                 n_dqn_nodes,  # K: number of DQN nodes\n",
    "                 n_nodes,  # N: number of all nodes\n",
    "                 n_actions,\n",
    "                 memory_size=500,\n",
    "                 replace_target_iter=200,\n",
    "                 batch_size=32,\n",
    "                 learning_rate=0.01,\n",
    "                 gamma=0.9,\n",
    "                 epsilon=1,\n",
    "                 epsilon_min=0.01,\n",
    "                 epsilon_decay=0.995,\n",
    "                 alpha=0  # 0 ~ 100 (inf)\n",
    "                 ):\n",
    "        # hyper-parameters\n",
    "        self.state_size = state_size\n",
    "        self.n_dqn_nodes = n_dqn_nodes\n",
    "        self.n_nodes = n_nodes\n",
    "        self.n_actions = n_actions\n",
    "        self.memory_size = memory_size\n",
    "        self.replace_target_iter = replace_target_iter\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def __create_agents__(self):\n",
    "        self.agents = []\n",
    "\n",
    "        for i in range(self.n_dqn_nodes):\n",
    "            dqn_agent = DQN(self.state_size,\n",
    "                            n_nodes=self.n_nodes,\n",
    "                            n_actions=self.n_actions,\n",
    "                            memory_size=self.memory_size,\n",
    "                            replace_target_iter=self.replace_target_iter,\n",
    "                            batch_size=self.batch_size,\n",
    "                            learning_rate=self.learning_rate,\n",
    "                            gamma=self.gamma,\n",
    "                            epsilon=self.epsilon,\n",
    "                            epsilon_min=self.epsilon_min,\n",
    "                            epsilon_decay=self.epsilon_decay,\n",
    "                            alpha=self.alpha\n",
    "                            )\n",
    "            self.agents.append(dqn_agent)\n",
    "\n",
    "    def reset(self):\n",
    "        self.__create_agents__()\n",
    "\n",
    "    def tic(self):\n",
    "        agent_actions = np.zeros(self.n_dqn_nodes, dtype=np.float32)\n",
    "\n",
    "        for i in range(self.n_dqn_nodes):\n",
    "            agent_actions[i] = self.agents[i].tic()\n",
    "        return agent_actions\n",
    "\n",
    "    def update(self, observations_, agent_rewards, non_agent_rewards):\n",
    "        for i in range(self.n_dqn_nodes):\n",
    "            # print(observations_[i], agent_rewards[i], non_agent_rewards[i])\n",
    "            self.agents[i].update(\n",
    "                observations_[i], agent_rewards[i], non_agent_rewards[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f8add5f",
   "metadata": {
    "papermill": {
     "duration": 0.004969,
     "end_time": "2023-06-14T19:23:43.298692",
     "exception": false,
     "start_time": "2023-06-14T19:23:43.293723",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## TDMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b44a327",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T19:23:43.310170Z",
     "iopub.status.busy": "2023-06-14T19:23:43.309444Z",
     "iopub.status.idle": "2023-06-14T19:23:43.317788Z",
     "shell.execute_reply": "2023-06-14T19:23:43.316959Z"
    },
    "id": "aIdP9E3zB2_g",
    "papermill": {
     "duration": 0.016107,
     "end_time": "2023-06-14T19:23:43.319748",
     "exception": false,
     "start_time": "2023-06-14T19:23:43.303641",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TDMA_NODES:\n",
    "    def __init__(self, n_nodes, action_list_len, X):\n",
    "        # n_actions=2: (wait, transmit)\n",
    "        # action_list_len and X indicate the parameters of ONE node.\n",
    "        self.n_nodes = n_nodes\n",
    "        self.action_list_len = action_list_len\n",
    "        self.X = X\n",
    "        self.action_list = self.__create_action_list__()\n",
    "        self.counter = 0\n",
    "\n",
    "    def __create_action_list__(self):  # (node, action_list)\n",
    "        action_list = np.zeros((self.n_nodes, self.action_list_len))\n",
    "        for i in range(self.n_nodes):\n",
    "            idx = np.random.choice(self.action_list_len, self.X, replace=False)\n",
    "            action_list[i, idx] = 1\n",
    "        return action_list\n",
    "\n",
    "    def tic(self):  # 1D: action of each node\n",
    "        tdma_action = self.action_list[:, self.counter]\n",
    "        # tdma_action = np.squeeze(tdma_action)\n",
    "        self.counter += 1\n",
    "        if self.counter == self.action_list.shape[1]:\n",
    "            self.counter = 0\n",
    "        return tdma_action.astype(np.float32)\n",
    "\n",
    "    def shuffle(self, _X):  # Change the action pattern.\n",
    "        self.X = _X\n",
    "        self.__create_action_list__()\n",
    "\n",
    "    def reset(self):\n",
    "        self.action_list = self.__create_action_list__()\n",
    "        self.counter = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b9ec330",
   "metadata": {
    "papermill": {
     "duration": 0.004815,
     "end_time": "2023-06-14T19:23:43.329524",
     "exception": false,
     "start_time": "2023-06-14T19:23:43.324709",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Exponential-backoff Aloha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b18aeac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T19:23:43.342066Z",
     "iopub.status.busy": "2023-06-14T19:23:43.340504Z",
     "iopub.status.idle": "2023-06-14T19:23:43.350901Z",
     "shell.execute_reply": "2023-06-14T19:23:43.350093Z"
    },
    "papermill": {
     "duration": 0.018279,
     "end_time": "2023-06-14T19:23:43.352815",
     "exception": false,
     "start_time": "2023-06-14T19:23:43.334536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EB_ALOHA_NODES:\n",
    "    def __init__(self, n_nodes, W=2, max_count=2):\n",
    "        # n_actions=2: (wait, transmit)\n",
    "        self.n_nodes = n_nodes\n",
    "        self.max_count = max_count\n",
    "        self.W = W\n",
    "        self.actions = np.zeros(self.n_nodes, dtype=np.float32)\n",
    "\n",
    "        self.count = np.zeros(self.n_nodes, dtype=np.float32)\n",
    "        self.backoff = np.random.randint(\n",
    "            0, self.W * 2**self.count, size=self.n_nodes)\n",
    "\n",
    "    def tic(self):\n",
    "        self.count = np.minimum(self.count, self.max_count)\n",
    "        self.backoff -= 1\n",
    "\n",
    "        filter_arr = self.backoff < 0\n",
    "        filter_arr = np.arange(self.n_nodes, dtype=np.int32)[filter_arr]\n",
    "        self.backoff[filter_arr] = np.random.randint(\n",
    "            0, self.W * 2**self.count)[filter_arr]\n",
    "\n",
    "        eb_Aloha_actions = (self.backoff == 0)\n",
    "        eb_Aloha_actions = eb_Aloha_actions.astype(np.float32)\n",
    "        self.actions = eb_Aloha_actions\n",
    "        return eb_Aloha_actions  # return 1 if timeout\n",
    "\n",
    "    def handle_success(self):\n",
    "        filter_arr = (self.actions == 1)\n",
    "        self.count[filter_arr] = np.zeros(self.n_nodes, dtype=np.int32)[filter_arr]\n",
    "        \n",
    "\n",
    "    def handle_collision(self):\n",
    "        filter_arr = (self.actions == 1)\n",
    "        self.count += filter_arr.astype(np.int32)\n",
    "\n",
    "    def reset(self):  # Change the action pattern.\n",
    "        self.count = np.zeros(self.n_nodes)\n",
    "        self.backoff = np.random.randint(\n",
    "            0, self.W * 2**self.count, size=self.n_nodes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2393f8cc",
   "metadata": {},
   "source": [
    "## q-Aloha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f1cc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class q_Aloha_NODES:\n",
    "    def __init__(self, n_nodes, q=0.5):\n",
    "        # n_actions=2: (wait, transmit)\n",
    "        self.n_nodes = n_nodes\n",
    "        assert (q <= 1 and q >= 0)\n",
    "        self.q = q  # probability to send\n",
    "        self.actions = np.zeros(self.n_nodes, dtype=np.float32)\n",
    "\n",
    "    def tic(self):\n",
    "        # return 1 with prob. q\n",
    "        return np.random.choice(2, self.n_nodes, p=[1-self.q, self.q])\n",
    "\n",
    "    def reset(self):  # Change the action pattern.\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "83ae7b19",
   "metadata": {
    "id": "bdSfcoDnQ2qM",
    "papermill": {
     "duration": 0.004822,
     "end_time": "2023-06-14T19:23:43.362630",
     "exception": false,
     "start_time": "2023-06-14T19:23:43.357808",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Environment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "836e0398",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51743ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.n_DQN = 2\n",
    "        self.n_TDMA = 0\n",
    "        self.n_EB_Aloha = 0\n",
    "        self.n_q_Aloha = 0\n",
    "\n",
    "        self.max_iter = 10000 # simulation iterations\n",
    "        self.N = 1000 # plot with avg of N iters\n",
    "\n",
    "        # Agent (DQN)\n",
    "        self.M = 20  # state length\n",
    "        self.E = 500  # memory size\n",
    "        self.F = 20  # target network update frequency\n",
    "        self.B = 32  # mini-batch size\n",
    "        self.alpha = 1 # alpha-fairness\n",
    "        # state = cat(s[8:], [action, observation, agent_reward, non_agent_reward])\n",
    "        self.state_size = int(8*self.M)\n",
    "\n",
    "        # TDMA\n",
    "        self.action_list_len = 10  # length of one period\n",
    "        self.X = 2  # number of slot used in one perios\n",
    "\n",
    "        # Exponential-backoff Aloha\n",
    "        # wnd = randint(0, W*2^count)\n",
    "        self.W = 2   # minimum window size\n",
    "        self.max_count = 2  # maximum backoff count\n",
    "        \n",
    "        # q-Aloha\n",
    "        self.q = .2\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72285d78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T19:23:43.374751Z",
     "iopub.status.busy": "2023-06-14T19:23:43.373738Z",
     "iopub.status.idle": "2023-06-14T19:23:43.384817Z",
     "shell.execute_reply": "2023-06-14T19:23:43.383993Z"
    },
    "id": "ZGZtSDMuQ2qM",
    "papermill": {
     "duration": 0.019039,
     "end_time": "2023-06-14T19:23:43.386736",
     "exception": false,
     "start_time": "2023-06-14T19:23:43.367697",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ENVIRONMENT:\n",
    "    def __init__(self, config):\n",
    "        self.__set_env__(config)\n",
    "\n",
    "    def __set_env__(self, _config):\n",
    "        self.n_DQN = _config.n_DQN\n",
    "        self.n_TDMA = _config.n_TDMA\n",
    "        self.n_EB_Aloha = _config. n_EB_Aloha\n",
    "        self.n_q_Aloha = _config.n_q_Aloha\n",
    "\n",
    "        self.n_nodes = self.n_DQN + self.n_TDMA + self.n_EB_Aloha + self.n_q_Aloha\n",
    "\n",
    "        self.dqn_nodes = DQN_NODES(_config.state_size,\n",
    "                                   n_dqn_nodes=self.n_DQN,\n",
    "                                   n_nodes=self.n_nodes,\n",
    "                                   n_actions=2,\n",
    "                                   memory_size=_config.E,\n",
    "                                   replace_target_iter=_config.F,\n",
    "                                   batch_size=_config.B,\n",
    "                                   learning_rate=0.01,\n",
    "                                   gamma=0.9,\n",
    "                                   epsilon=0.5,\n",
    "                                   epsilon_min=0.005,\n",
    "                                   epsilon_decay=0.995,\n",
    "                                   alpha=_config.alpha\n",
    "                                   )\n",
    "        self.tdma_nodes = TDMA_NODES(\n",
    "            _config.n_TDMA, _config.action_list_len, _config.X)\n",
    "        self.EB_ALOHA_NODES = EB_ALOHA_NODES(\n",
    "            _config.n_EB_Aloha, _config.W, _config.max_count)\n",
    "        self.q_Aloha_nodes = q_Aloha_NODES(_config.n_q_Aloha, _config.q)\n",
    "\n",
    "    def reset(self, _config):\n",
    "        self.config = _config\n",
    "        self.__set_env__(self.config)\n",
    "\n",
    "        self.dqn_nodes.reset()\n",
    "        self.tdma_nodes.reset()\n",
    "        self.EB_ALOHA_NODES.reset()\n",
    "        self.q_Aloha_nodes.reset()\n",
    "\n",
    "    def step(self):\n",
    "        dqn_rewards = np.zeros(self.n_DQN)\n",
    "        tdma_rewards = np.zeros(self.n_TDMA)\n",
    "        eb_Aloha_rewards = np.zeros(self.n_EB_Aloha)\n",
    "        q_Aloha_rewards = np.zeros(self.n_q_Aloha)\n",
    "\n",
    "        dqn_actions = np.zeros(self.n_DQN, dtype=np.float32)\n",
    "        tdma_actions = np.zeros(self.n_TDMA, dtype=np.float32)\n",
    "        eb_Aloha_actions = np.zeros(self.n_EB_Aloha, dtype=np.float32)\n",
    "        q_Aloha_actions = np.zeros(self.n_q_Aloha, dtype=np.float32)\n",
    "\n",
    "        observation_ = np.array(['I']*self.n_DQN)  # obersvation for DQN nodes\n",
    "\n",
    "        if self.n_DQN > 0:\n",
    "            dqn_actions = self.dqn_nodes.tic()\n",
    "        if self.n_TDMA > 0:\n",
    "            tdma_actions = self.tdma_nodes.tic()\n",
    "        if self.n_EB_Aloha > 0:\n",
    "            eb_Aloha_actions = self.EB_ALOHA_NODES.tic()\n",
    "        if self.n_q_Aloha > 0:\n",
    "            q_Aloha_actions = self.q_Aloha_nodes.tic()\n",
    "\n",
    "        # evaluate media condition\n",
    "        n_Tx = np.sum(dqn_actions)+np.sum(tdma_actions) + \\\n",
    "            np.sum(eb_Aloha_actions)+np.sum(q_Aloha_actions)\n",
    "        assert n_Tx >= 0\n",
    "\n",
    "        if n_Tx == 0:  # idle (default)\n",
    "            pass\n",
    "        elif n_Tx == 1:  # success Tx\n",
    "            dqn_rewards = dqn_actions\n",
    "            tdma_rewards = tdma_actions\n",
    "            eb_Aloha_rewards = eb_Aloha_actions\n",
    "            q_Aloha_rewards = q_Aloha_actions\n",
    "\n",
    "            self.EB_ALOHA_NODES.handle_success()\n",
    "\n",
    "            for i in range(self.n_DQN):\n",
    "                observation_[i] = 'S' if dqn_actions[i] == 1 else 'B'\n",
    "        else:  # collision\n",
    "            self.EB_ALOHA_NODES.handle_collision()\n",
    "\n",
    "            if np.sum(dqn_actions) > 0:\n",
    "                for i in range(self.n_DQN):\n",
    "                    observation_[i] = 'F' if dqn_actions[i] == 1 else 'B'\n",
    "            else:\n",
    "                observation_ = np.array(['B']*self.n_DQN)\n",
    "\n",
    "        # update DQN nodes\n",
    "        non_agent_rewards = np.zeros(\n",
    "            (self.n_DQN, self.n_nodes-1), dtype=np.float32)\n",
    "        cat_rewards = np.concatenate(\n",
    "            (dqn_rewards, tdma_rewards, eb_Aloha_rewards, q_Aloha_rewards), dtype=np.float32)\n",
    "\n",
    "        for i in range(self.n_DQN):\n",
    "            non_agent_rewards[i, :i] = cat_rewards[np.newaxis, :i]\n",
    "            non_agent_rewards[i, i:] = cat_rewards[np.newaxis, i+1:]\n",
    "            \n",
    "        self.dqn_nodes.update(observation_, dqn_rewards, non_agent_rewards)\n",
    "\n",
    "        return dqn_rewards, tdma_rewards, eb_Aloha_rewards, q_Aloha_rewards"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88e1d045",
   "metadata": {
    "id": "TWDuuALSQ2qN",
    "papermill": {
     "duration": 0.00486,
     "end_time": "2023-06-14T19:23:43.396636",
     "exception": false,
     "start_time": "2023-06-14T19:23:43.391776",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Run DQN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab098820",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T19:23:46.717486Z",
     "iopub.status.busy": "2023-06-14T19:23:46.717170Z",
     "iopub.status.idle": "2023-06-14T19:23:46.728980Z",
     "shell.execute_reply": "2023-06-14T19:23:46.727944Z"
    },
    "id": "ox4ptx1GQ2qN",
    "papermill": {
     "duration": 0.02067,
     "end_time": "2023-06-14T19:23:46.731435",
     "exception": false,
     "start_time": "2023-06-14T19:23:46.710765",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = ENVIRONMENT(config=config)\n",
    "\n",
    "def main(config):\n",
    "    agent_reward_list = []\n",
    "    tdma_reward_list = []\n",
    "    eb_Aloha_reward_list = []\n",
    "    q_Aloha_reward_list = []\n",
    "\n",
    "    M, E, F, B, X, W, q = config.M, config.E, config.F, config.B, config.X, config.W, config.q\n",
    "    n_DQN, n_TDMA, n_EB_Aloha, n_q_Aloha = config.n_DQN, config.n_TDMA, config.n_EB_Aloha, config.n_q_Aloha\n",
    "    max_iter = config.max_iter\n",
    "\n",
    "    for i in tqdm(range(config.max_iter)):\n",
    "        dqn_rewards, tdma_rewards, eb_Aloha_rewards, q_Aloha_rewards = env.step()\n",
    "\n",
    "        agent_reward_list.append(dqn_rewards)\n",
    "        tdma_reward_list.append(tdma_rewards)\n",
    "        eb_Aloha_reward_list.append(eb_Aloha_rewards)\n",
    "        q_Aloha_reward_list.append(q_Aloha_rewards)\n",
    "\n",
    "    agent_arr = np.array(agent_reward_list, dtype=np.float32)\n",
    "    tdma_arr = np.array(tdma_reward_list, dtype=np.float32)\n",
    "    eb_Aloha_arr = np.array(eb_Aloha_reward_list, dtype=np.float32)\n",
    "    q_Aloha_arr = np.array(q_Aloha_reward_list, dtype=np.float32)\n",
    "\n",
    "    if not os.path.isdir('./rewards'):\n",
    "        os.mkdir('./rewards')\n",
    "    file_path = f'rewards/{EXPERIMENT_NAME}_rewards_dqn{n_DQN}_t{n_TDMA}_ea{n_EB_Aloha}_qa{n_q_Aloha}_M{M}_E{E:.0E}_F{F}_B{B}_X{X}_W{W}_q{q}_{max_iter:.0E}.npz'\n",
    "    np.savez(file_path, agent=agent_arr, tdma=tdma_arr,\n",
    "             eb_Aloha=eb_Aloha_arr, q_Aloha=q_Aloha_arr)\n",
    "\n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ff2fa0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T19:23:46.743772Z",
     "iopub.status.busy": "2023-06-14T19:23:46.743468Z",
     "iopub.status.idle": "2023-06-14T19:41:16.385890Z",
     "shell.execute_reply": "2023-06-14T19:41:16.385022Z"
    },
    "id": "IzAbJ_XyQ2qO",
    "papermill": {
     "duration": 1049.651166,
     "end_time": "2023-06-14T19:41:16.388292",
     "exception": false,
     "start_time": "2023-06-14T19:23:46.737126",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = main(config=config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "203b483a",
   "metadata": {
    "id": "eTEze5yDQ2qP",
    "papermill": {
     "duration": 2.679892,
     "end_time": "2023-06-14T19:41:21.691821",
     "exception": false,
     "start_time": "2023-06-14T19:41:19.011929",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Average_throughput\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9836f8c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T19:41:27.482732Z",
     "iopub.status.busy": "2023-06-14T19:41:27.481658Z",
     "iopub.status.idle": "2023-06-14T19:41:27.494963Z",
     "shell.execute_reply": "2023-06-14T19:41:27.494087Z"
    },
    "id": "FLbAOcM8Q2qP",
    "papermill": {
     "duration": 2.664918,
     "end_time": "2023-06-14T19:41:27.496931",
     "exception": false,
     "start_time": "2023-06-14T19:41:24.832013",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_avg_throughput(file, config):\n",
    "    max_iter = config.max_iter\n",
    "    N = config.N\n",
    "    n_DQN = config.n_DQN\n",
    "    n_TDMA = config.n_TDMA\n",
    "    n_EB_Aloha = config.n_EB_Aloha\n",
    "    n_q_Aloha = config.n_q_Aloha\n",
    "\n",
    "    num = [n_DQN, n_TDMA, n_EB_Aloha, n_q_Aloha]\n",
    "    category = ['agent', 'tdma', 'eb_Aloha', 'q_Aloha']\n",
    "\n",
    "    # load reward\n",
    "    data = np.load(file)\n",
    "\n",
    "    labels = []\n",
    "    rewards = np.zeros((sum(num), max_iter), dtype=np.float32)  # reward\n",
    "\n",
    "    cnt = 0\n",
    "    for i in range(len(category)):\n",
    "        _data = np.transpose(data[category[i]])\n",
    "        for n in range(num[i]):\n",
    "            lbl = f'{category[i]} {n+1}' if num[i] > 1 else f'{category[i]}'\n",
    "            labels.append(lbl)\n",
    "            rewards[cnt, :] = _data[n]\n",
    "            cnt += 1\n",
    "\n",
    "    avg_throughput = np.zeros((sum(num), max_iter), dtype=np.float32)\n",
    "    temp_sum = np.zeros((sum(num), 1), dtype=np.float32)\n",
    "\n",
    "    for i in range(0, max_iter):\n",
    "        if i < N:\n",
    "            temp_sum[:, 0] += rewards[:, i]\n",
    "            avg_throughput[:, i] = temp_sum[:, 0]/(i+1)\n",
    "        else:\n",
    "            temp_sum[:, 0] += rewards[:, i]-rewards[:, i-N]\n",
    "            avg_throughput[:, i] = temp_sum[:, 0]/N\n",
    "\n",
    "    avg_throughput_total = np.sum(avg_throughput, axis=0, dtype=np.float32)\n",
    "\n",
    "    plt.xlim((0, max_iter))\n",
    "    plt.ylim((-0.05, 1))\n",
    "\n",
    "    legend_list = []\n",
    "\n",
    "    for i in range(len(avg_throughput)):\n",
    "        line, = plt.plot(avg_throughput[i], lw=1, label=labels[i])\n",
    "        legend_list.append(line)\n",
    "\n",
    "    total_line, = plt.plot(avg_throughput_total,\n",
    "                           color='r', lw=1.5, label='total')\n",
    "    legend_list.append(total_line)\n",
    "\n",
    "    plt.grid()\n",
    "    plt.legend(handles=legend_list, loc='best')\n",
    "    plt.xlabel(\"iteration\")\n",
    "    plt.ylabel(\"average throughput\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27797789",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T19:41:32.751277Z",
     "iopub.status.busy": "2023-06-14T19:41:32.750916Z",
     "iopub.status.idle": "2023-06-14T19:41:33.059042Z",
     "shell.execute_reply": "2023-06-14T19:41:33.058190Z"
    },
    "id": "vjJ1t9fAQ2qP",
    "papermill": {
     "duration": 2.91987,
     "end_time": "2023-06-14T19:41:33.061040",
     "exception": false,
     "start_time": "2023-06-14T19:41:30.141170",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig1 = plt.figure()\n",
    "plot_avg_throughput(file_path, config)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1094.949721,
   "end_time": "2023-06-14T19:41:38.977737",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-06-14T19:23:24.028016",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
