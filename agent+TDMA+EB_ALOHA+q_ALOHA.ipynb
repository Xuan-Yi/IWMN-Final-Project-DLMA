{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaffb2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T19:23:35.060999Z",
     "iopub.status.busy": "2023-06-14T19:23:35.060569Z",
     "iopub.status.idle": "2023-06-14T19:23:43.188468Z",
     "shell.execute_reply": "2023-06-14T19:23:43.187376Z"
    },
    "id": "ZH0EFBY1Q2qF",
    "papermill": {
     "duration": 8.138434,
     "end_time": "2023-06-14T19:23:43.191548",
     "exception": false,
     "start_time": "2023-06-14T19:23:35.053114",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Add\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.initializers import glorot_normal\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c0bce2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T19:23:43.203453Z",
     "iopub.status.busy": "2023-06-14T19:23:43.202835Z",
     "iopub.status.idle": "2023-06-14T19:23:43.207080Z",
     "shell.execute_reply": "2023-06-14T19:23:43.206245Z"
    },
    "id": "rNfH9X9nCQZj",
    "papermill": {
     "duration": 0.012306,
     "end_time": "2023-06-14T19:23:43.209128",
     "exception": false,
     "start_time": "2023-06-14T19:23:43.196822",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = 'agent+q_ALOHA(Adam)'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb2860ec",
   "metadata": {
    "id": "6CcCuj8JQEA_",
    "papermill": {
     "duration": 0.004806,
     "end_time": "2023-06-14T19:23:43.218811",
     "exception": false,
     "start_time": "2023-06-14T19:23:43.214005",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Fix random seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e971e212",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T19:23:43.229595Z",
     "iopub.status.busy": "2023-06-14T19:23:43.229290Z",
     "iopub.status.idle": "2023-06-14T19:23:43.234076Z",
     "shell.execute_reply": "2023-06-14T19:23:43.233100Z"
    },
    "id": "M5ivJuAQQCx3",
    "papermill": {
     "duration": 0.012309,
     "end_time": "2023-06-14T19:23:43.235958",
     "exception": false,
     "start_time": "2023-06-14T19:23:43.223649",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def same_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "same_seeds(48763)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd8b6037",
   "metadata": {
    "id": "7dlFkYONQ2qJ",
    "papermill": {
     "duration": 0.004748,
     "end_time": "2023-06-14T19:23:43.245575",
     "exception": false,
     "start_time": "2023-06-14T19:23:43.240827",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Protocols\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7885fc5f",
   "metadata": {
    "papermill": {
     "duration": 0.004673,
     "end_time": "2023-06-14T19:23:43.255086",
     "exception": false,
     "start_time": "2023-06-14T19:23:43.250413",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49b6487",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T19:23:43.266482Z",
     "iopub.status.busy": "2023-06-14T19:23:43.266187Z",
     "iopub.status.idle": "2023-06-14T19:23:43.286954Z",
     "shell.execute_reply": "2023-06-14T19:23:43.286052Z"
    },
    "id": "C0DeJwXCQ2qK",
    "papermill": {
     "duration": 0.028631,
     "end_time": "2023-06-14T19:23:43.288834",
     "exception": false,
     "start_time": "2023-06-14T19:23:43.260203",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self,\n",
    "                 state_size,\n",
    "                 n_actions,\n",
    "                 memory_size=500,\n",
    "                 replace_target_iter=200,\n",
    "                 batch_size=32,\n",
    "                 learning_rate=0.01,\n",
    "                 gamma=0.9,\n",
    "                 epsilon=1,\n",
    "                 epsilon_min=0.01,\n",
    "                 epsilon_decay=0.995\n",
    "                 ):\n",
    "        # hyper-parameters\n",
    "        self.state_size = state_size\n",
    "        self.n_actions = n_actions\n",
    "        self.memory_size = memory_size\n",
    "        self.replace_target_iter = replace_target_iter\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.memory = np.zeros((self.memory_size, self.state_size * 2 + 2))\n",
    "        # temporary parameters\n",
    "        self.learn_step_counter = 0\n",
    "        self.memory_couter = 0\n",
    "\n",
    "        # build model\n",
    "        self.model = self.build_ResNet_model()  # model: evaluate Q value\n",
    "        self.target_model = self.build_ResNet_model()  # target_mode: target network\n",
    "\n",
    "    def build_ResNet_model(self):\n",
    "        inputs = Input(shape=(self.state_size, ))\n",
    "        h1 = Dense(64, activation=\"relu\",\n",
    "                   kernel_initializer='glorot_normal')(inputs)  # h1\n",
    "        h2 = Dense(64, activation=\"relu\",\n",
    "                   kernel_initializer='glorot_normal')(h1)  # h2\n",
    "\n",
    "        h3 = Dense(64, activation=\"relu\",\n",
    "                   kernel_initializer='glorot_normal')(h2)  # h3\n",
    "        h4 = Dense(64, activation=\"relu\",\n",
    "                   kernel_initializer='glorot_normal')(h3)  # h4\n",
    "        add1 = Add()([h4, h2])\n",
    "\n",
    "        h5 = Dense(64, activation=\"relu\",\n",
    "                   kernel_initializer='glorot_normal')(add1)  # h5\n",
    "        h6 = Dense(64, activation=\"relu\",\n",
    "                   kernel_initializer='glorot_normal')(h5)  # h6\n",
    "        add2 = Add()([h6, add1])\n",
    "\n",
    "        outputs = Dense(\n",
    "            self.n_actions, kernel_initializer='glorot_normal')(add2)\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        # model.compile(loss=\"mse\", optimizer=RMSprop(\n",
    "        #     learning_rate=self.learning_rate))\n",
    "        model.compile(loss=\"mse\", optimizer=Adam(\n",
    "            learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        # Apply epsilon-greedy algorithm\n",
    "        state = state[np.newaxis, :]\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon)\n",
    "\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(0, self.n_actions)\n",
    "\n",
    "        action_values = self.model.predict(state, verbose=None)\n",
    "        return np.argmax(action_values)\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        # s_: next_state\n",
    "        # r: ndarray or int\n",
    "        if np.isscalar(r):\n",
    "            r = [r]\n",
    "        if not hasattr(self, 'memory_couter'):\n",
    "            self.memory_couter = 0\n",
    "        transition = np.concatenate((s, [a], r, s_))\n",
    "        index = self.memory_couter % self.memory_size\n",
    "        self.memory[index, :] = transition\n",
    "        self.memory_couter += 1\n",
    "\n",
    "    def repalce_target_parameters(self):\n",
    "        weights = self.model.get_weights()\n",
    "        self.target_model.set_weights(weights)\n",
    "\n",
    "    def learn(self):\n",
    "        # check to update target netowrk parameters\n",
    "        if self.learn_step_counter % self.replace_target_iter == 0:\n",
    "            self.repalce_target_parameters()  # iterative target model\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "        # sample batch memory from all memory\n",
    "        if self.memory_couter > self.memory_size:\n",
    "            sample_index = np.random.choice(\n",
    "                self.memory_size, size=self.batch_size)\n",
    "        else:\n",
    "            sample_index = np.random.choice(\n",
    "                self.memory_couter, size=self.batch_size)\n",
    "        batch_memory = self.memory[sample_index, :]\n",
    "\n",
    "        # batch memory row: [s, a, r, s_]\n",
    "        # number of batch memory: batch size\n",
    "        state = batch_memory[:, :self.state_size]\n",
    "        action = batch_memory[:, self.state_size].astype(int)  # float -> int\n",
    "        reward = batch_memory[:, self.state_size+1]\n",
    "        next_state = batch_memory[:, -self.state_size:]\n",
    "\n",
    "        q = self.model.predict(state, verbose=None)  # state\n",
    "        q_targ = self.target_model.predict(\n",
    "            next_state, verbose=None)  # next state\n",
    "\n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "        q[batch_index, action] = reward + self.gamma * np.max(q_targ, axis=1)\n",
    "\n",
    "        self.model.fit(state, q, self.batch_size, epochs=1, verbose=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f8add5f",
   "metadata": {
    "papermill": {
     "duration": 0.004969,
     "end_time": "2023-06-14T19:23:43.298692",
     "exception": false,
     "start_time": "2023-06-14T19:23:43.293723",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## TDMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b44a327",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T19:23:43.310170Z",
     "iopub.status.busy": "2023-06-14T19:23:43.309444Z",
     "iopub.status.idle": "2023-06-14T19:23:43.317788Z",
     "shell.execute_reply": "2023-06-14T19:23:43.316959Z"
    },
    "id": "aIdP9E3zB2_g",
    "papermill": {
     "duration": 0.016107,
     "end_time": "2023-06-14T19:23:43.319748",
     "exception": false,
     "start_time": "2023-06-14T19:23:43.303641",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TDMA:\n",
    "    def __init__(self, n_nodes, action_list_len, X):\n",
    "        # n_actions=2: (wait, transmit)\n",
    "        # action_list_len and X indicate the parameters of ONE node.\n",
    "        self.n_nodes = n_nodes\n",
    "        self.action_list_len = action_list_len\n",
    "        self.X = X\n",
    "        self.action_list = self.__create_action_list__()\n",
    "        self.counter = 0\n",
    "\n",
    "    def __create_action_list__(self):  # (node, action_list)\n",
    "        action_list = np.zeros((self.n_nodes, self.action_list_len))\n",
    "        for i in range(self.n_nodes):\n",
    "            idx = np.random.choice(self.action_list_len, self.X, replace=False)\n",
    "            action_list[i, idx] = 1\n",
    "        return action_list\n",
    "\n",
    "    def tic(self):  # 1D: action of each node\n",
    "        tdma_action = self.action_list[:, self.counter]\n",
    "        # tdma_action = np.squeeze(tdma_action)\n",
    "        self.counter += 1\n",
    "        if self.counter == self.action_list.shape[1]:\n",
    "            self.counter = 0\n",
    "        return tdma_action.astype(np.float32)\n",
    "\n",
    "    def shuffle(self, _X):  # Change the action pattern.\n",
    "        self.X = _X\n",
    "        self.__create_action_list__()\n",
    "\n",
    "    def reset(self):\n",
    "        self.action_list = self.__create_action_list__()\n",
    "        self.counter = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b9ec330",
   "metadata": {
    "papermill": {
     "duration": 0.004815,
     "end_time": "2023-06-14T19:23:43.329524",
     "exception": false,
     "start_time": "2023-06-14T19:23:43.324709",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Exponential-backoff ALOHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b18aeac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T19:23:43.342066Z",
     "iopub.status.busy": "2023-06-14T19:23:43.340504Z",
     "iopub.status.idle": "2023-06-14T19:23:43.350901Z",
     "shell.execute_reply": "2023-06-14T19:23:43.350093Z"
    },
    "papermill": {
     "duration": 0.018279,
     "end_time": "2023-06-14T19:23:43.352815",
     "exception": false,
     "start_time": "2023-06-14T19:23:43.334536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EB_ALOHA:\n",
    "    def __init__(self, n_nodes, W=2, max_count=2):\n",
    "        # n_actions=2: (wait, transmit)\n",
    "        self.n_nodes = n_nodes\n",
    "        self.max_count = max_count\n",
    "        self.W = W\n",
    "        self.actions = np.zeros(self.n_nodes, dtype=np.float32)\n",
    "\n",
    "        self.count = np.zeros(self.n_nodes, dtype=np.float32)\n",
    "        self.backoff = np.random.randint(\n",
    "            0, self.W * 2**self.count, size=self.n_nodes)\n",
    "\n",
    "    def tic(self):\n",
    "        self.count = np.minimum(self.count, self.max_count)\n",
    "        self.backoff -= 1\n",
    "\n",
    "        filter_arr = self.backoff < 0\n",
    "        filter_arr = np.arange(self.n_nodes, dtype=np.int32)[filter_arr]\n",
    "        self.backoff[filter_arr] = np.random.randint(\n",
    "            0, self.W * 2**self.count)[filter_arr]\n",
    "\n",
    "        eb_aloha_actions = (self.backoff == 0)\n",
    "        eb_aloha_actions = eb_aloha_actions.astype(np.float32)\n",
    "        self.actions = eb_aloha_actions\n",
    "        return eb_aloha_actions  # return 1 if timeout\n",
    "\n",
    "    def handle_success(self):\n",
    "        filter_arr = (self.actions == 1)\n",
    "        self.count[filter_arr] = np.zeros(self.n_nodes, dtype=np.int32)[filter_arr]\n",
    "        \n",
    "\n",
    "    def handle_collision(self):\n",
    "        filter_arr = (self.actions == 1)\n",
    "        self.count += filter_arr.astype(np.int32)\n",
    "\n",
    "    def reset(self):  # Change the action pattern.\n",
    "        self.count = np.zeros(self.n_nodes)\n",
    "        self.backoff = np.random.randint(\n",
    "            0, self.W * 2**self.count, size=self.n_nodes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2393f8cc",
   "metadata": {},
   "source": [
    "## q-ALOHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f1cc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class q_ALOHA:\n",
    "    def __init__(self, n_nodes, q=0.5):\n",
    "        # n_actions=2: (wait, transmit)\n",
    "        self.n_nodes = n_nodes\n",
    "        assert (q <= 1 and q >= 0)\n",
    "        self.q = q  # probability to send\n",
    "        self.actions = np.zeros(self.n_nodes, dtype=np.float32)\n",
    "\n",
    "    def tic(self):\n",
    "        # return 1 with prob. q\n",
    "        return np.random.choice(2, self.n_nodes, p=[1-self.q, self.q])\n",
    "\n",
    "    def reset(self):  # Change the action pattern.\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "83ae7b19",
   "metadata": {
    "id": "bdSfcoDnQ2qM",
    "papermill": {
     "duration": 0.004822,
     "end_time": "2023-06-14T19:23:43.362630",
     "exception": false,
     "start_time": "2023-06-14T19:23:43.357808",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72285d78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T19:23:43.374751Z",
     "iopub.status.busy": "2023-06-14T19:23:43.373738Z",
     "iopub.status.idle": "2023-06-14T19:23:43.384817Z",
     "shell.execute_reply": "2023-06-14T19:23:43.383993Z"
    },
    "id": "ZGZtSDMuQ2qM",
    "papermill": {
     "duration": 0.019039,
     "end_time": "2023-06-14T19:23:43.386736",
     "exception": false,
     "start_time": "2023-06-14T19:23:43.367697",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ENVIRONMENT:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.set_env(self.config)\n",
    "    \n",
    "    def set_env(self, config_):\n",
    "        self.n_TDMA = config_.n_TDMA\n",
    "        self.n_EB_ALOHA = config_. n_EB_ALOHA\n",
    "        self.n_q_ALOHA = config_.n_q_ALOHA\n",
    "        \n",
    "        self.TDMA_nodes = TDMA(config_.n_TDMA, config_.action_list_len, config_.X)\n",
    "        self.EB_ALOHA_nodes = EB_ALOHA(\n",
    "            config_.n_EB_ALOHA, config_.W, config_.max_count)\n",
    "        self.q_ALOHA_nodes = q_ALOHA(config_.n_q_ALOHA, config_.q)\n",
    "        \n",
    "        self.state_size = config_.state_size\n",
    "        \n",
    "    def reset(self):\n",
    "        self.TDMA_nodes.reset()\n",
    "        self.EB_ALOHA_nodes.reset()\n",
    "        self.q_ALOHA_nodes.reset()\n",
    "        \n",
    "        init_state = np.zeros(self.state_size)\n",
    "        return init_state\n",
    "\n",
    "    def step(self, action):\n",
    "        agent_reward = 0\n",
    "        tdma_reward = np.zeros(self.n_TDMA)\n",
    "        eb_aloha_reward = np.zeros(self.n_EB_ALOHA)\n",
    "        q_aloha_reward = np.zeros(self.n_q_ALOHA)\n",
    "\n",
    "        tdma_actions = np.zeros(self.n_TDMA, dtype=np.float32)\n",
    "        eb_aloha_actions = np.zeros(self.n_EB_ALOHA, dtype=np.float32)\n",
    "        q_aloha_actions = np.zeros(self.n_q_ALOHA, dtype=np.float32)\n",
    "\n",
    "        reward = 0\n",
    "        observation_ = 0\n",
    "\n",
    "        if self.n_TDMA > 0:\n",
    "            tdma_actions = self.TDMA_nodes.tic()\n",
    "        if self.n_EB_ALOHA > 0:\n",
    "            eb_aloha_actions = self.EB_ALOHA_nodes.tic()\n",
    "        if self.n_q_ALOHA > 0:\n",
    "            q_aloha_actions = self.q_ALOHA_nodes.tic()\n",
    "\n",
    "        if action == 1:\n",
    "            if np.sum(tdma_actions)+np.sum(eb_aloha_actions)+np.sum(q_aloha_actions) > 0:  # collision\n",
    "                observation_ = 'F'  # tx, failed\n",
    "                self.EB_ALOHA_nodes.handle_collision()\n",
    "            else:  # agent success\n",
    "                reward = 1.0\n",
    "                agent_reward = 1.0\n",
    "                observation_ = 'S'  # tx, success\n",
    "        else:\n",
    "            other_tx = np.sum(tdma_actions) + \\\n",
    "                np.sum(eb_aloha_actions)+np.sum(q_aloha_actions)\n",
    "\n",
    "            if other_tx == 0:  # idle\n",
    "                observation_ = 'I'\n",
    "            elif other_tx == 1:  # media busy\n",
    "                reward = 1.0\n",
    "                tdma_reward = tdma_actions\n",
    "                eb_aloha_reward = eb_aloha_actions\n",
    "                q_aloha_reward = q_aloha_actions\n",
    "                observation_ = 'B'\n",
    "                self.EB_ALOHA_nodes.handle_success()\n",
    "            else:  # some nodes collide\n",
    "                reward = 1.0\n",
    "                observation_ = 'B'\n",
    "                self.EB_ALOHA_nodes.handle_collision()\n",
    "\n",
    "        return observation_, reward, agent_reward, tdma_reward, eb_aloha_reward, q_aloha_reward"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88e1d045",
   "metadata": {
    "id": "TWDuuALSQ2qN",
    "papermill": {
     "duration": 0.00486,
     "end_time": "2023-06-14T19:23:43.396636",
     "exception": false,
     "start_time": "2023-06-14T19:23:43.391776",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Run DQN\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "836e0398",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51743ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.n_DQN = 1\n",
    "        self.n_TDMA = 3\n",
    "        self.n_EB_ALOHA = 0\n",
    "        self.n_q_ALOHA = 0\n",
    "\n",
    "        self.max_iter = 10000 # simulation iterations\n",
    "        self.N = 1000 # plot with avg of N iters\n",
    "\n",
    "        # Agent (DLMA)\n",
    "        self.M = 20  # state length\n",
    "        self.E = 500  # memory size\n",
    "        self.F = 20  # target network update frequency\n",
    "        self.B = 32  # mini-batch size\n",
    "        # state = cat(s[8:], [action, observation, agent_reward, sum of others' reward])\n",
    "        self.state_size = int(8*self.M)\n",
    "\n",
    "        # TDMA\n",
    "        self.action_list_len = 10  # length of one period\n",
    "        self.X = 5  # number of slot used in one perios\n",
    "\n",
    "        # Exponential-backoff ALOHA\n",
    "        # wnd = randint(0, W*2^count)\n",
    "        self.W = 2   # minimum window size\n",
    "        self.max_count = 2  # maximum backoff count\n",
    "        \n",
    "        # q-ALOHA\n",
    "        self.q = .3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11cc99a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T19:23:43.409368Z",
     "iopub.status.busy": "2023-06-14T19:23:43.407823Z",
     "iopub.status.idle": "2023-06-14T19:23:46.702811Z",
     "shell.execute_reply": "2023-06-14T19:23:46.701804Z"
    },
    "papermill": {
     "duration": 3.303359,
     "end_time": "2023-06-14T19:23:46.705196",
     "exception": false,
     "start_time": "2023-06-14T19:23:43.401837",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = Config()\n",
    "\n",
    "env = ENVIRONMENT(config=config)\n",
    "\n",
    "dqn_agent = DQN(env.state_size,\n",
    "                n_actions=2,\n",
    "                memory_size=config.E,\n",
    "                replace_target_iter=config.F,\n",
    "                batch_size=config.B,\n",
    "                learning_rate=0.01,\n",
    "                gamma=0.9,\n",
    "                epsilon=0.5,\n",
    "                epsilon_min=0.005,\n",
    "                epsilon_decay=0.995,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab098820",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T19:23:46.717486Z",
     "iopub.status.busy": "2023-06-14T19:23:46.717170Z",
     "iopub.status.idle": "2023-06-14T19:23:46.728980Z",
     "shell.execute_reply": "2023-06-14T19:23:46.727944Z"
    },
    "id": "ox4ptx1GQ2qN",
    "papermill": {
     "duration": 0.02067,
     "end_time": "2023-06-14T19:23:46.731435",
     "exception": false,
     "start_time": "2023-06-14T19:23:46.710765",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def return_action(action, n_actions=2):\n",
    "    one_hot_vector = [0] * n_actions\n",
    "    one_hot_vector[action] = 1\n",
    "    return one_hot_vector\n",
    "\n",
    "\n",
    "def return_observation(o):\n",
    "    if o == 'S':\n",
    "        return [1, 0, 0, 0]\n",
    "    elif o == 'F':\n",
    "        return [0, 1, 0, 0]\n",
    "    elif o == 'B':\n",
    "        return [0, 0, 1, 0]\n",
    "    elif o == 'I':\n",
    "        return [0, 0, 0, 1]\n",
    "\n",
    "\n",
    "def main(config):\n",
    "    agent_reward_list = []\n",
    "    tdma_reward_list = []\n",
    "    eb_aloha_reward_list = []\n",
    "    q_aloha_reward_list = []\n",
    "    state = env.reset()\n",
    "\n",
    "    M, E, F, B, X, W, q = config.M, config.E, config.F, config.B, config.X, config.W, config.q\n",
    "    n_TDMA, n_EB_ALOHA, n_q_ALOHA = config.n_TDMA, config.n_EB_ALOHA, config.n_q_ALOHA\n",
    "    max_iter = config.max_iter\n",
    "\n",
    "    for i in tqdm(range(config.max_iter)):\n",
    "        agent_action = dqn_agent.choose_action(state)\n",
    "        observation_, reward, agent_reward, tdma_reward, eb_aloha_reward, q_aloha_reward = env.step(\n",
    "            agent_action)\n",
    "\n",
    "        agent_reward_list.append(agent_reward)\n",
    "        tdma_reward_list.append(tdma_reward)\n",
    "        eb_aloha_reward_list.append(eb_aloha_reward)\n",
    "        q_aloha_reward_list.append(q_aloha_reward)\n",
    "\n",
    "        # state = (action_t, observation_t)\n",
    "        next_state = np.concatenate((state[8:], np.array(return_action(\n",
    "            agent_action) + return_observation(observation_) + [agent_reward, np.sum(tdma_reward)+np.sum(eb_aloha_reward)+np.sum(q_aloha_reward)], dtype=np.float32)))\n",
    "        dqn_agent.store_transition(state, agent_action, reward, next_state)\n",
    "        if i > 100:\n",
    "            dqn_agent.learn()    # internally iterates default (prediction) model\n",
    "        state = next_state\n",
    "\n",
    "    agent_arr = np.array(agent_reward_list, dtype=np.float32)\n",
    "    tdma_arr = np.array(tdma_reward_list, dtype=np.float32)\n",
    "    eb_aloha_arr = np.array(eb_aloha_reward_list, dtype=np.float32)\n",
    "    q_aloha_arr = np.array(q_aloha_reward_list, dtype=np.float32)\n",
    "\n",
    "    if not os.path.isdir('./rewards'):\n",
    "        os.mkdir('./rewards')\n",
    "    file_path = f'rewards/{EXPERIMENT_NAME}_rewards_t{n_TDMA}_ea{n_EB_ALOHA}_qa{n_q_ALOHA}_M{M}_E{E:.0E}_F{F}_B{B}_X{X}_W{W}_q{q}_{max_iter:.0E}.npz'\n",
    "    np.savez(file_path, agent=agent_arr, tdma=tdma_arr,\n",
    "             eb_aloha=eb_aloha_arr, q_aloha=q_aloha_arr)\n",
    "\n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ff2fa0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T19:23:46.743772Z",
     "iopub.status.busy": "2023-06-14T19:23:46.743468Z",
     "iopub.status.idle": "2023-06-14T19:41:16.385890Z",
     "shell.execute_reply": "2023-06-14T19:41:16.385022Z"
    },
    "id": "IzAbJ_XyQ2qO",
    "papermill": {
     "duration": 1049.651166,
     "end_time": "2023-06-14T19:41:16.388292",
     "exception": false,
     "start_time": "2023-06-14T19:23:46.737126",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = main(config=config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "203b483a",
   "metadata": {
    "id": "eTEze5yDQ2qP",
    "papermill": {
     "duration": 2.679892,
     "end_time": "2023-06-14T19:41:21.691821",
     "exception": false,
     "start_time": "2023-06-14T19:41:19.011929",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Average_throughput\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9836f8c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T19:41:27.482732Z",
     "iopub.status.busy": "2023-06-14T19:41:27.481658Z",
     "iopub.status.idle": "2023-06-14T19:41:27.494963Z",
     "shell.execute_reply": "2023-06-14T19:41:27.494087Z"
    },
    "id": "FLbAOcM8Q2qP",
    "papermill": {
     "duration": 2.664918,
     "end_time": "2023-06-14T19:41:27.496931",
     "exception": false,
     "start_time": "2023-06-14T19:41:24.832013",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_avg_throughput(file):\n",
    "    max_iter = config.max_iter\n",
    "    N = config.N\n",
    "\n",
    "    # load reward\n",
    "    reward = np.load(file)\n",
    "\n",
    "    avg_throughput_agent = np.zeros((1, max_iter))\n",
    "    avg_throughput_tdma = np.zeros((1, max_iter))\n",
    "    avg_throughput_eb_aloha = np.zeros((1, max_iter))\n",
    "    avg_throughput_q_aloha = np.zeros((1, max_iter))\n",
    "\n",
    "    agent_temp_sum = 0\n",
    "    tdma_temp_sum = 0\n",
    "    eb_aloha_temp_sum = 0\n",
    "    q_aloha_temp_sum = 0\n",
    "\n",
    "    # print(reward['tdma'].size, reward['eb_aloha'].size, reward['q_aloha'].size)\n",
    "    agent_reward = reward['agent']\n",
    "    tdma_reward = np.sum(\n",
    "        reward['tdma'], axis=1) if reward['tdma'].size != 0 else np.zeros(max_iter)\n",
    "    eb_aloha_reward = np.sum(\n",
    "        reward['eb_aloha'], axis=1) if reward['eb_aloha'].size != 0 else np.zeros(max_iter)\n",
    "    q_aloha_reward = np.sum(\n",
    "        reward['q_aloha'], axis=1) if reward['q_aloha'].size != 0 else np.zeros(max_iter)\n",
    "\n",
    "    for i in range(0, max_iter):\n",
    "        if i < N:\n",
    "            agent_temp_sum += agent_reward[i]\n",
    "            avg_throughput_agent[0][i] = agent_temp_sum / (i+1)\n",
    "            tdma_temp_sum += tdma_reward[i]\n",
    "            avg_throughput_tdma[0][i] = tdma_temp_sum / (i+1)\n",
    "            eb_aloha_temp_sum += eb_aloha_reward[i]\n",
    "            avg_throughput_eb_aloha[0][i] = eb_aloha_temp_sum / (i+1)\n",
    "            q_aloha_temp_sum += q_aloha_reward[i]\n",
    "            avg_throughput_q_aloha[0][i] = q_aloha_temp_sum / (i+1)\n",
    "        else:\n",
    "            agent_temp_sum += agent_reward[i] - agent_reward[i-N]\n",
    "            avg_throughput_agent[0][i] = agent_temp_sum / N\n",
    "            tdma_temp_sum += tdma_reward[i] - tdma_reward[i-N]\n",
    "            avg_throughput_tdma[0][i] = tdma_temp_sum / N\n",
    "            eb_aloha_temp_sum += eb_aloha_reward[i] - eb_aloha_reward[i-N]\n",
    "            avg_throughput_eb_aloha[0][i] = eb_aloha_temp_sum / N\n",
    "            q_aloha_temp_sum += q_aloha_reward[i] - q_aloha_reward[i-N]\n",
    "            avg_throughput_q_aloha[0][i] = q_aloha_temp_sum / N\n",
    "\n",
    "    # Total throughput\n",
    "    avg_throughput_total = avg_throughput_agent + avg_throughput_tdma + \\\n",
    "        avg_throughput_eb_aloha + avg_throughput_q_aloha\n",
    "\n",
    "    plt.xlim((0, max_iter))\n",
    "    plt.ylim((-0.05, 1))\n",
    "\n",
    "    legend_list = []\n",
    "\n",
    "    agent_line, = plt.plot(\n",
    "        avg_throughput_agent[0], color='c', lw=1, label='agent')\n",
    "    legend_list.append(agent_line)\n",
    "\n",
    "    if config.n_TDMA > 0:\n",
    "        tdma_line, = plt.plot(\n",
    "            avg_throughput_tdma[0], color='m', lw=1, label='tdma')\n",
    "        legend_list.append(tdma_line)\n",
    "\n",
    "    if config.n_EB_ALOHA > 0:\n",
    "        eb_aloha_line, = plt.plot(\n",
    "            avg_throughput_eb_aloha[0], color='y', lw=1, label='eb-aloha')\n",
    "        legend_list.append(eb_aloha_line)\n",
    "\n",
    "    if config.n_q_ALOHA > 0:\n",
    "        q_aloha_line, = plt.plot(\n",
    "            avg_throughput_q_aloha[0], color='g', lw=1, label='q-aloha')\n",
    "        legend_list.append(q_aloha_line)\n",
    "\n",
    "    total_line, = plt.plot(\n",
    "        avg_throughput_total[0], color='r', lw=1.5, label='total')\n",
    "    legend_list.append(total_line)\n",
    "\n",
    "    plt.grid()\n",
    "    plt.legend(handles=legend_list, loc='best')\n",
    "    plt.xlabel(\"iteration\")\n",
    "    plt.ylabel(\"average throughput\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27797789",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T19:41:32.751277Z",
     "iopub.status.busy": "2023-06-14T19:41:32.750916Z",
     "iopub.status.idle": "2023-06-14T19:41:33.059042Z",
     "shell.execute_reply": "2023-06-14T19:41:33.058190Z"
    },
    "id": "vjJ1t9fAQ2qP",
    "papermill": {
     "duration": 2.91987,
     "end_time": "2023-06-14T19:41:33.061040",
     "exception": false,
     "start_time": "2023-06-14T19:41:30.141170",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig1 = plt.figure()\n",
    "plot_avg_throughput(file_path)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1094.949721,
   "end_time": "2023-06-14T19:41:38.977737",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-06-14T19:23:24.028016",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
